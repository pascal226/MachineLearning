---
title: "Machine Learning Project"
author: "Pascal Grabbe"
date: "19 Juni 2018"
output: html_document
---

## Set working directory and load data:

```{r}
setwd("D://R Coursera/Machine Learning")

fileURL1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
destfile="./traindata.csv" 
download.file(fileURL1 , destfile, method="auto")
traindata <- read.csv("traindata.csv", sep = ",", header = TRUE)

fileURL2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
destfile="./testdata1.csv" 
download.file(fileURL2 , destfile, method="auto")
testdata <- read.csv("testdata1.csv", sep = ",", header = TRUE)
```
Load needed libraries:
```{r, warning = FALSE, include=TRUE, cache=FALSE, results=FALSE}
library(caret)
library(randomForest)
```
Split the training set in train and test set:
```{r}
inTrain  <- createDataPartition(y = traindata$classe, p = 0.75, list = F)
train    <- traindata[inTrain,]
test     <- traindata[-inTrain,]
dim(train); dim(test);dim(testdata)
```
Exclude variables with close to zero variance which are meaningless for out model and check if all three sets have the same column count:
```{r}
NZV      <- nearZeroVar(train)
train    <- train[, -NZV]
test     <- test[, -NZV]
testdata <- testdata[, -NZV]
dim(train);dim(test);dim(testdata)
```
Exclude variables with zero values and check again the variable number on all three datasets:
```{r}
train          <- train[,colSums(is.na(train)) == 0]
test           <- test[,colSums(is.na(test)) == 0]
testdata       <- testdata[,colSums(is.na(testdata)) == 0]
dim(train);dim(test);dim(testdata)
```
Exclude the first six variables wich have also no meaning for our model, also check the dimensions again:
```{r, echo=T}
train      = train[, -c(1:6)]
test       = test [, -c(1:6)]
testdata   = testdata [, -c(1:6)]
dim(train);dim(test);dim(testdata)
```
## First model Random Forest
We do simple cross validation with three resampling iterations and apply our fitted model to the test set:
```{r, echo=T}
set.seed(365)
controlRF  <- trainControl(method="cv", number=3, verboseIter=FALSE)
modFitRF   <- train(classe ~ ., data=train, method="rf", trControl=controlRF)
sol        <- predict(modFitRF, newdata = test)                          
solconf    <- confusionMatrix(sol, test$classe)
solconf
```
```{r, echo=T, include=FALSE}
plot(solconf$table, main = "Rondom Forest")
```

With an accuracy of .9941 our model seems to be quite good, so we fit it to the test set:
```{r, echo=T}
predictRF  <- predict(modFitRF, testdata)
predictRF
```
## Second model Generalized Boosted Model
Just try another model to compare our result. We now use boosting with trees:
```{r, echo=T, message=FALSE}
set.seed(555)
controlGBM <- trainControl(method="repeatedcv", number=3, verboseIter=TRUE)
PCFit      <- train(classe ~., data = train, method ="gbm", trControl=controlGBM)
sol2       <- predict(PCFit, newdata = test)
sol2conf   <- confusionMatrix(sol2, test$classe)
sol2conf
```
```{r, echo=T, include=FALSE}
plot(sol2conf$table, main = "Boosting with Trees")
```

With an accuracy of 0.9631 boosting with trees is slightly worse than random forest, although it produces the same predictions:
```{r, echo=T}
predictGBM <- predict(PCFit, testdata)
predictGBM
```
